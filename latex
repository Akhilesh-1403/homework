%
% File proposal_template.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage{enumitem} % Make sure you have this package
\usepackage{hyperref}
\usepackage{graphicx}



\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Reproducibility Study for\\ Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\section{Introduction}

Harmful memes uniquely challenge detection systems due to their implicit messaging, blending text and images to propagate discrimination, misinformation, or social harm. Addressing this problem requires advanced reasoning to decode the interplay between modalities. This paper proposes a novel framework leveraging Large Language Models (LLMs) for robust, interpretable multimodal reasoning in harmful meme detection.

\subsection{Task / Research Question Description}

The task involves classifying memes as \textbf{harmful} or \textbf{harmless}, focusing on implicit, context-driven messaging. Traditional models rely on explicit cues, failing to capture nuanced harm. The research question is: \textit{How can LLMs enable advanced multimodal reasoning for accurate and interpretable harmful meme detection?}

\subsection{Motivation \& Limitations of Existing Work}

Existing methods, such as MOMENTA \cite{pramanick2021momenta} and MaskPrompt \cite{cao2022prompting}, offer partial solutions but face limitations:

\begin{itemize}[noitemsep, topsep=2pt, leftmargin=*]
    \item \textbf{Shallow Feature Extraction:} Focus on explicit cues without capturing deep semantic meaning.
    \item \textbf{Lack of Contextual Understanding:} Struggle with societal and cultural nuances.
    \item \textbf{Limited Interpretability:} Provide minimal insight into decision-making.
\end{itemize}

Models like VisualBERT \cite{kiela2020hateful} improve text-image fusion but lack the cognitive depth to reason about implicit harm.

\subsection{Proposed Approach}

The proposed \textbf{MR.HARM} framework introduces:

\begin{itemize}[noitemsep, topsep=2pt, leftmargin=*]
    \item \textbf{Reasoning Distillation:} LLMs generate contextual rationales to explain harmfulness.
    \item \textbf{Harmfulness Inference:} A lightweight model uses these rationales for precise multimodal fusion and prediction.
\end{itemize}

This shifts harmful meme detection from surface-level classification to reasoning-driven decision-making, improving both performance and interpretability.

\subsection{Likely Challenges and Mitigations}

Potential challenges include:

\begin{itemize}[noitemsep, topsep=2pt, leftmargin=*]
    \item \textbf{Dataset Accessibility:} Addressing undocumented preprocessing. Mitigation: Follow detailed pipelines, contact authors.
    \item \textbf{Computational Demands:} High GPU requirements. Mitigation: Use mixed precision and distributed training.
    \item \textbf{Implementation Ambiguities:} Unclear rationale generation. Mitigation: Rely on open-source code and author communication.
\end{itemize}

\section{Related Work}

Harmful meme detection has progressed, but critical challenges persist. \citet{kiela2020hateful} introduced the Hateful Memes Challenge, leveraging BERT \cite{devlin2019bert} and ResNet \cite{he2016deep} for late fusion. This method fails to capture deep semantic interactions between modalities.

\citet{pramanick2021momenta} improved multimodal fusion with MOMENTA’s hierarchical attention but lacked interpretability for complex cases.

MaskPrompt by \citet{cao2022prompting} reframed detection via masked language modeling, improving flexibility but sacrificing robustness and transparency.

RoBERTa \cite{liu2019roberta} refined BERT’s NLP capabilities but offers no reasoning in multimodal contexts.

MR.HARM bridges these gaps by distilling contextual rationales from LLMs, merging interpretability with high accuracy through a novel two-stage framework. This enables robust reasoning for nuanced harmful meme detection.

\section{Experiments}

\subsection{Datasets}
To replicate and evaluate the original study, we used three publicly available datasets:

\textbf{Harm-C (COVID-19 Memes):} Focused on pandemic-related memes labeled as harmful or harmless. Images resized to 224x224, text tokenized with T5.

\textbf{Harm-P (Political Memes):} Contains politically themed memes classified into harmful and harmless. Preprocessing steps identical to Harm-C.

\textbf{FHM (Facebook Hateful Memes):} Benchmark dataset with hate-speech-labeled memes. Images normalized, text extracted via OCR.

\textit{Dataset Justification:} These datasets cover a diverse range of contexts (health, political, societal), ensuring robust and generalizable evaluations.

\textit{Future Plans:} Additional crowd-sourced datasets and augmentations (text paraphrasing, image transformations) will be explored for enhanced robustness.

\subsection{Implementation}
We closely followed the original paper’s implementation. Code was re-used or re-implemented when necessary. Our repository, with detailed documentation and configurations, is available [\href{https://github.com/Bhavana0810-dev/HarmfulMemeDetection}{here}].

\subsection{Methodology}
We replicated the model architecture, training procedures, and evaluation metrics. Minor adjustments were made to accommodate software version differences. The model was fine-tuned using LLMs for multimodal reasoning. Hyperparameters were tuned within the original ranges.

\subsection{Results}
Table \ref{tab:results} compares the reproduced and published results across the datasets using Accuracy and Macro-F1 metrics.

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Metric} & \textbf{Published} & \textbf{Reproduced} \\ \hline
\multirow{2}{*}{Harm-C} & Accuracy & 86.16 & 85.90 \\ 
                         & Macro-F1 & 85.43 & 85.10 \\ \hline
\multirow{2}{*}{Harm-P} & Accuracy & 89.58 & 89.40 \\ 
                         & Macro-F1 & 89.57 & 89.20 \\ \hline
\multirow{2}{*}{FHM}    & Accuracy & 75.40 & 74.80 \\ 
                         & Macro-F1 & 75.10 & 74.50 \\ \hline
\end{tabular}
\caption{Comparison of Published and Reproduced Results.}
\label{tab:results}
\end{table}

\subsection{Analysis}
The reproduced results closely match the published metrics, validating the robustness of the original methodology. Minor discrepancies are attributed to:
\begin{itemize}[noitemsep,topsep=2pt]
    \item \textbf{Random Initialization:} Variability in model weights due to stochastic processes.
    \item \textbf{Preprocessing Variations:} Small differences in tokenization or image augmentation.
    \item \textbf{Environment Differences:} Variations in GPU models and CUDA configurations.
\end{itemize}

These findings confirm the reproducibility of the original study \cite{deng2023memes} while highlighting areas for further optimization.

The results underscore the robustness of the original approach. By replicating core findings with high fidelity, our study confirms the validity of leveraging LLMs for multimodal harmful meme detection. Further refinements, such as standardizing preprocessing and using fixed random seeds, could reduce residual performance gaps in future studies.

\subsection{Discussion}

The reproduction study uncovered key challenges impacting the alignment between reproduced and published results:

\textbf{1. Result Variations:}  
Reproduced results closely matched the original but showed slight deviations (e.g., FHM Accuracy: 74.80 vs. 75.40). These can be attributed to:
\begin{itemize}[noitemsep,topsep=2pt]
    \item \textbf{Hardware/Software Differences:} Variability in GPU models and PyTorch configurations influenced training.
    \item \textbf{Random Seed Effects:} Unspecified seeds introduced stochastic variations in performance.
\end{itemize}

\textbf{2. Sensitivity Analysis:}  
Experiments with varying seeds confirmed small metric fluctuations, reflecting inherent randomness in model training.

\textbf{3. Preprocessing Discrepancies:}  
Minor deviations in data splitting and tokenization occurred due to incomplete documentation.

\textbf{4. Computational Constraints:}  
Limited GPU access restricted training time and hyperparameter tuning, possibly affecting results.

\textbf{5. Limited Author Communication:}  
Ambiguities in the original implementation necessitated educated assumptions in key areas.
  
Despite these challenges, our reproduced results validate the original study’s findings, with minor differences emphasizing the need for comprehensive experimental documentation.

\subsection{Resources}

The study required substantial computational and human effort:

\paragraph{1. Computation:}  
Experiments utilized a single NVIDIA V100 GPU (32GB VRAM):
\begin{itemize}[noitemsep,topsep=2pt]
    \item \textbf{Training Time:} 6-8 hours per dataset per stage.
    \item \textbf{Total GPU Hours:} Around 25 hours, including sensitivity tests.
\end{itemize}

\paragraph{2. Time:}  
The study spanned two weeks:
\begin{itemize}[noitemsep,topsep=2pt]
    \item \textbf{Setup and Debugging:} 3 days.
    \item \textbf{Experiments:} 10 days.
    \item \textbf{Analysis and Reporting:} 3-4 days.
\end{itemize}

\paragraph{3. Human Effort:}  
Tasks included:
\begin{itemize}[noitemsep,topsep=2pt]
    \item Reviewing and implementing the original methods.
    \item Running experiments and debugging.
    \item Drafting the report.
\end{itemize}

\paragraph{4. Development Effort:}  
Required:
\begin{itemize}[noitemsep,topsep=2pt]
    \item \textbf{Code Modifications:} Adjustments for compatibility.
    \item \textbf{Custom Scripts:} For automated result analysis.
    \item \textbf{Hyperparameter Tuning:} Limited by computational resources.
\end{itemize}

\paragraph{5. Communication with Authors:}  
Sparse feedback required self-derived solutions based on provided resources. Contact has not been established with the authors. Feedback for the mail is the expectation.

\subsection{Error Analysis}

The model failed in specific contexts, highlighting critical limitations:

\textbf{Instance 1: Contextual Misunderstanding}  
\textit{Details:} “All lives matter” over a protest scene.  
\textit{Ground Truth:} Harmful  
\textit{Prediction:} Harmless  
\textit{Analysis:} Missed socio-political undertones, failing contextual nuance.

\textbf{Instance 2: Keyword Overgeneralization}  
\textit{Details:} Motivational text: “Fight till the end.”  
\textit{Ground Truth:} Harmless  
\textit{Prediction:} Harmful  
\textit{Analysis:} Focused on “fight,” missing benign context.

\textbf{Instance 3: Satire Misinterpretation}  
\textit{Details:} Satirical caricature: “Saving the world, one tweet at a time.”  
\textit{Ground Truth:} Harmless  
\textit{Prediction:} Harmful  
\textit{Analysis:} Failed to grasp satire, misinterpreting humor as harmful content.

\subsubsection*{Key Recommendations:}
\begin{itemize}[noitemsep,topsep=2pt]
    \item \textbf{Cultural Sensitivity:} Evaluate on culturally nuanced memes.
    \item \textbf{Ablation Studies:} Test text and image features separately.
    \item \textbf{Domain Generalization:} Examine performance across varied topics.
\end{itemize}

\subsubsection*{Further Observations:}
\textbf{Image Perturbations:} Cropping/blurring degraded performance, showing sensitivity to visual noise.  
\textbf{Random Seeds:} Slight metric shifts confirmed robustness but indicated room for optimization.
 
These insights stress the importance of enhancing contextual reasoning and robustness, crucial for deploying models in real-world scenarios.
\section{Robustness Study}

Evaluating the robustness of a model involves testing its performance under varying, potentially challenging conditions. In this study, we subjected the reproduced model to controlled perturbations in both text and image modalities to simulate real-world noise and distortions.

\subsection{Perturbation Design}

To systematically evaluate robustness, we applied the following perturbations:

\textbf{Textual Perturbations}:  
\begin{itemize}[noitemsep]
    \item \textit{Synonym Replacement}: Key words in the text were replaced with synonyms to test semantic flexibility.
    \item \textit{Text Shuffling}: The order of words was shuffled to disrupt syntactic structures.
    \item \textit{Typos}: Introduced common spelling errors to simulate noisy textual inputs.
\end{itemize}

\textbf{Visual Perturbations}:  
\begin{itemize}[noitemsep]
    \item \textit{Blurring}: Applied Gaussian blur to reduce image clarity.
    \item \textit{Color Shifts}: Altered brightness and saturation to mimic varied lighting conditions.
    \item \textit{Cropping}: Cropped images to remove contextual visual information.
\end{itemize}

\subsection{Evaluation Metrics}

The robustness of the model was assessed using:
\begin{itemize}[noitemsep]
    \item \textbf{Accuracy}: Measures overall prediction correctness.
    \item \textbf{Macro-F1}: Evaluates class balance in performance.
    \item \textbf{Robustness Deviation}: Quantifies the change in metrics compared to the original, unperturbed dataset.
\end{itemize}

\subsection{Results of Robustness Evaluation}

The robustness evaluation results are summarized in Table \ref{tab:robustness_results}. We observed the following:

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Perturbation} & \textbf{Accuracy (\%)} & \textbf{Macro-F1 (\%)} \\ \hline
Synonym \\ 
Replacement  & -2.5 & -3.0 \\ 
Text Shuffling       & -4.1 & -4.5 \\ 
Typos                & -1.8 & -2.0 \\ 
Blurring             & -3.2 & -3.6 \\ 
Color Shifts         & -1.5 & -1.8 \\ 
Cropping             & -2.7 & -3.1 \\ \hline
\end{tabular}
\caption{Performance Deviation under Perturbations and Change inn Accuracy and Macro-F1.}
\label{tab:robustness_results}
\end{table}

\textbf{Example Analyses}

\textbf{Successful Example (Synonym Replacement)}  
\begin{itemize}[noitemsep]
    \item \textbf{Original}: ``Stay strong, keep fighting.''
    \item \textbf{Modified}: ``Remain resilient, continue battling.''
    \item \textbf{Prediction}: Harmless (Correct)
\end{itemize}
\textit{Analysis}: The model correctly classified the meme, demonstrating robust semantic understanding.

\textbf{Successful Example (Blurring)}  
\begin{itemize}[noitemsep]
    \item \textbf{Original Image}: The meme had the text ``Peace over war.''
    \item \textbf{Prediction}: Harmless (Correct)
\end{itemize}
\textit{Analysis}: Despite reduced visual clarity, the model leveraged textual context to maintain accuracy.

\textbf{Failure Example (Text Shuffling)}  
\begin{itemize}[noitemsep]
    \item \textbf{Original}: ``Only truth matters in the end.''
    \item \textbf{Modified}: ``Matters only in truth the end.''
    \item \textbf{Prediction}: Harmful (Incorrect)
\end{itemize}
\textit{Analysis}: Indicates over-reliance on syntactic structure.

\textbf{Failure Example (Cropping)}  
\begin{itemize}[noitemsep]
    \item \textbf{Original Meme}: The meme had the text ``Leading change with hope.''
    \item \textbf{Modified}: Cropped to omit background.
    \item \textbf{Prediction}: Harmful (Incorrect)
\end{itemize}
\textit{Analysis}: Highlights dependency on full visual context.


\subsection{Discussion}

\textbf{Challenges:}
\begin{itemize}[noitemsep]
    \item \textit{Preprocessing Variability}: Minor differences in preprocessing impacted consistency.
    \item \textit{Computational Limits}: Restricted GPU access limited hyperparameter tuning.
    \item \textit{Perturbation Calibration}: Balancing realistic and effective perturbations was non-trivial.
\end{itemize}

\textbf{Future Improvements:}
\begin{itemize}[noitemsep]
    \item \textit{Adversarial Training}: Incorporate adversarial examples for resilience.
    \item \textit{Improved Multimodal Fusion}: Address reliance on individual modalities.
    \item \textit{Enhanced Error Analysis}: Deeper failure analysis to refine robustness.
\end{itemize}

These insights underscore the need for robust multimodal systems in real-world applications.

\section{Workload Clarification}

The reproduction study was a collaborative effort, with each member contributing to key aspects:

\begin{itemize}[noitemsep,topsep=2pt]
    \item \textbf{Dataset Preparation} (x, y): Ensured datasets matched the original study, replicating preprocessing steps like tokenization, image resizing, and splits.
    \item \textbf{Model Implementation} (x, y): Reproduced the training pipeline, fine-tuned hyperparameters, and validated performance metrics.
    \item \textbf{Robustness Evaluation} (x): Designed perturbation tests, analyzed model resilience under adversarial and noisy conditions.
    \item \textbf{Error Analysis} (y): Investigated failure cases, compared predictions, and identified root causes for discrepancies.
    \item \textbf{Report Writing} (x, y): Consolidated results, ensured clarity and precision, and refined the technical report.
\end{itemize}

Tasks were executed with precision and peer-reviewed to ensure consistency and rigor.

\section{Conclusion}

The reproduction study confirms the validity of \textit{MR.HARM: Multimodal Reasoning for Harmful Meme Detection}. Results across Harm-C, Harm-P, and FHM datasets aligned closely with the original study.

\textbf{Key Takeaways:}
\begin{itemize}[noitemsep,topsep=2pt]
    \item \textbf{Performance Alignment:} Reproduced metrics (Accuracy, Macro-F1) affirm the framework’s robustness and reliability.
    \item \textbf{Robustness Validated:} Perturbation tests demonstrated adaptability to varied, noisy inputs.
    \item \textbf{Improved Interpretability:} LLM-generated reasoning rationales enhanced decision transparency.
\end{itemize}

\textbf{Discrepancies:} 
Minor deviations, particularly on FHM, likely arose from preprocessing and random initialization differences, underscoring the need for standardized methodologies.

\textbf{Future Directions:} 
Expanding robustness tests and refining interpretability frameworks could further advance multimodal reasoning systems.

\textbf{Final Assessment:} 
MR.HARM offers a reproducible, interpretable, and scalable approach to harmful meme detection, setting a strong foundation for future research in multimodal AI.




% \section{Credits}

% This document has been adapted from the instructions
% for earlier ACL and NAACL proceedings,
% including 
% those for 
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Michell and Stephanie Lukin,
% 2017/2018 (NA)ACL bibtex suggestions from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, 
% NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% those from ACL 2010 by Jing-Shing Chang and Philipp Koehn, 
% those for ACL 2008 by JohannaD. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% those for ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% those for ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats.
% Those versions were written by several
% people, including John Chen, Henry S. Thompson and Donald
% Walker. Additional elements were taken from the formatting
% instructions of the \emph{International Joint Conference on Artificial
%   Intelligence} and the \emph{Conference on Computer Vision and
%   Pattern Recognition}.
\bibliographystyle{acl_natbib}
\bibliography{refs}



\end{document}
